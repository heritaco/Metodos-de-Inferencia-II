---
title: "Trabajo Final Complementario"
output:     
  html_document:
    theme: darkly
    highlight: breezedark
---

<style>

h1.title {
  margin: 50px 20px 50px 20px;
  font-size: 3em;
  font-weight: bold;
  text-align: center;
}

.portada {
  max-width: 800px;
  margin: 100px auto;
  text-align: center;
  font-size: 1.2em;
}

.trabalo {
  max-width: 800px;
  margin: 100px auto;
  font-size: 1.2em;
  text-align: justify;
}

</style>

---

<div class="portada">
  
  Andrea Hernandez y Heriberto Espino  
  Universidad de las Américas Puebla  
  O24 LAT3062 1: Métodos de Inferencia II  
  4 de diciembre de 2024
  
</div>

---     


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  comment = "",
  warning = FALSE,
  fig.align = 'center'
)
```

<div class="trabalo">

# **Análisis de Patrones de Compra**

Se busca investigar si los hombres tienen menos intención de compra que las mujeres, utilizando los rangos derivados de sus puntajes en una escala de compra.

<br>
<center>
\( H_0: \)
Las distribuciones de los puntajes de intención de compra son iguales para hombres y mujeres.

**\(vs\)**

\( H_a: \)
Los hombres tienen menor intención de compra que las mujeres (puntajes estocásticamente menores).

</center>
<br>

Los rangos están ya para una prueba de Wilcoxon. Haciendo los calculos obtenemos lo siguiente:


```{r 01}

Rango_1 = c(40.5, 3.5, 3.5, 19.5, 30, 40.5, 10.5, 30, 19.5, 19.5, 3.5, 19.5, 19.5, 40.5, 10.5, 10.5, 30, 10.5, 10.5, 10.5, 3.5, 40.5, 30, 10.5)
Rango_2 = c(40.5, 19.5, 40.5, 30, 10.5, 19.5, 40.5, 30, 3.5, 40.5, 30, 3.5, 40.5, 30, 30, 40.5, 19.5, 19.5, 19.5, 30, 10.5)

Sum_r1 = sum(Rango_1)
Sum_r2 = sum(Rango_2)

n1 = length(Rango_1)
n2 = length(Rango_2)

E_x = n1 * (n1 + n2 + 1) / 2
Var_x = n1 * n2 * (n1 + n2 + 1) / 12

U_x = (Sum_r1 - E_x) / sqrt(Var_x)

p_value = pnorm(U_x)

cat("Valor de n_x:", n1, "\nValor de n_y:", n2, "\nValor de W_x:", Sum_r1, "\nValor de W_y:", Sum_r2,
    "\nValor de E_x:", E_x, "\nValor de Var_x:", Var_x, "\nW ~ N(", E_x, ",", Var_x, ")\nValor de Z:", U_x, "\nValor de p:", p_value)

```


**Suma de rangos**:


Hombres \( R_h = 467 \)

Mujeres \( R_m =  548.5 \)

La suma de rangos de los hombres es menor que la de las mujeres. Esto sugiere, preliminarmente, que los hombres podrían tener menor intención de compra en comparación con las mujeres.

### Prueba Utilizada: **Wilcoxon Rank-Sum Test**

p-valor obtenido: **\( p = 0.02657 \)**

A un nivel de significancia del 5%, \( \alpha = 0.05 \).
\( p = 0.02657 < \alpha = 0.05 \), se **rechaza \( H_0 \)**.


Con un valor \( p = 0.02657 \), existe suficiente evidencia para concluir que los hombres tienen menor intención de compra que las mujeres en este estudio. Esto apoya la hipótesis alternativa de que los puntajes de los hombres son estocásticamente menores.


La prueba de Wilcoxon resulta adecuada, dado que no se asume normalidad en las distribuciones de los puntajes. Este resultado podría tener implicaciones prácticas en estrategias de marketing orientadas por género.

---



























<br>


# **Evaluacion del Rendimiento de Pruebas de Normalidad**

Evaluaremos el rendimiento de diferentes pruebas de normalidad mediante simulaciones.
Se comparan las pruebas de Lilliefors, Cramér-von Mises, Anderson-Darling y Shapiro-Wilk
para distintos tamaños de muestra (30, 120, 480) y bajo hipótesis nula verdadera y falsa.
Las simulaciones se ejecutan 10,000 veces con un nivel de significancia de 0.10.
Los resultados incluyen proporciones de rechazos y distribuciones de valores p
para cada prueba, permitiendo analizar su efectividad en la detección de normalidad.

<br>
<center>
**\( H_0 \) Verdadera:**
Las muestras provienen de una distribución normal \( \mathcal{N}(\mu, \sigma^2) \).

**\(vs\)**

**\( H_0 \) Falsa:**
Las muestras no provienen de una distribución normal.

</center>
<br>


### **Tamaños de muestra y simulaciones**

1. Tamaños de muestra: \( n = 30 \), \( n = 120 \), \( n = 480 \).

2. Repeticiones: \( B = 10,000 \).

3. Significancia: \( \alpha = 0.10 \).

Se generan \( B \) muestras de tamaño \( n \) según el escenario (\( H_0 \) verdadera o falsa) y se aplican las pruebas LT, CVMT, AD, SFT, y SWT y registrar proporciones de rechazo de \( H_0 \) para cada prueba.

```{r}
library(nortest)
library(moments)
library(ggplot2)
library(stats)    

set.seed(24)

# Parameteros
sample_sizes <- c(30, 120, 480)
B <- 1000
alpha <- 0.10

# Resultados
results <- list(
  H0_true = list(),
  H0_false = list()
)

# Funcion de Simulacion
simulate_tests <- function(n, dist, test_funcs, B, alpha) {
  rejections <- matrix(0, nrow = B, ncol = length(test_funcs))
  colnames(rejections) <- names(test_funcs)
  
  for (i in 1:B) {
    if (dist == "normal") {
      data <- rnorm(n)
    } else {
      data <- rt(n, df = 5)
    }
    for (test in names(test_funcs)) {
      test_result <- tryCatch(test_funcs[[test]](data), error = function(e) NULL)
      if (!is.null(test_result) && test_result$p.value < alpha) {
        rejections[i, test] <- 1
      }
    }
  }
  colMeans(rejections)
}

# Definir estadisticas
test_functions <- list(
  LT = lillie.test,
  CVMT = cvm.test,
  AD = ad.test,
  SFT = shapiro.test
)

# Simulaciones para H0 verdadero
for (n in sample_sizes) {
  results$H0_true[[as.character(n)]] <- simulate_tests(n, "normal", test_functions, B, alpha)
}

# Crear tablas
library(knitr)
kable(as.data.frame(results$H0_true), digits = 4,  align = 'ccr', caption = "H0 Verdadero: Proporcion de Rechazos")

# Plot p-valores
n_example <- 30
p_values_true <- replicate(B, {
  data <- rnorm(n_example)
  sapply(test_functions, function(f) f(data)$p.value)
})
p_values_false <- replicate(B, {
  data <- rt(n_example, df = 5)
  sapply(test_functions, function(f) f(data)$p.value)
})

p_df_true <- data.frame(t(p_values_true))
p_df_true$Test <- rownames(p_df_true)
p_df_true_melt <- reshape2::melt(p_df_true, id.vars = "Test")

p_df_false <- data.frame(t(p_values_false))
p_df_false$Test <- rownames(p_df_false)
p_df_false_melt <- reshape2::melt(p_df_false, id.vars = "Test")
```

Ejemplo gráfico para $B = 30$.

```{r}
B <- 30

# Initialize results
results <- list(
  H0_true = list(),
  H0_false = list()
)

test_functions <- list(
  LT = lillie.test,
  CVMT = cvm.test,
  AD = ad.test,
  SFT = shapiro.test
)

# Run simulations for H0_true
for (n in sample_sizes) {
  results$H0_true[[as.character(n)]] <- simulate_tests(n, "normal", test_functions, B, alpha)
}

# Crear tablas
# Plot p-valores
n_example <- 30
p_values_true <- replicate(B, {
  data <- rnorm(n_example)
  sapply(test_functions, function(f) f(data)$p.value)
})
p_values_false <- replicate(B, {
  data <- rt(n_example, df = 5)
  sapply(test_functions, function(f) f(data)$p.value)
})

p_df_true <- data.frame(t(p_values_true))
p_df_true$Test <- rownames(p_df_true)
p_df_true_melt <- reshape2::melt(p_df_true, id.vars = "Test")

p_df_false <- data.frame(t(p_values_false))
p_df_false$Test <- rownames(p_df_false)
p_df_false_melt <- reshape2::melt(p_df_false, id.vars = "Test")

ggplot(p_df_true_melt, aes(x = value, fill = Test)) +
  geom_density(alpha = 0.5) +
  labs(title = paste("Distribución de P-Valores H0 Verdadera n =", n_example), x = "P-Valor") +
  theme_minimal()

```



### **Ganador cuando $H_0$ se cumple**

Cuando la hipótesis nula se cumple, las pruebas que tienen una proporción menor de falsos positivos son (en promedio) el **Test de Anderson-Darling (AD)**, seguido de el **Test de Shapiro-Wilk (SFT)**, rechazando un poco más del 10%. Estas pruebas rechazan menos H_0 para un 10% de significancia en distintos tamaños de muestra, con proporciones muy similares entre sí. Aunque aumente el tamaño de muestra (n=30, n=480), las proporciones de rechazo no muestran un cambio significativo. No se observan patrones relevantes en las proporciones de rechazo al aumentar el tamaño de la muestra, lo que indica que las fluctuaciones entre tamaños de muestra son menores y probablemente debidas a variabilidad aleatoria.

### **Ganador cuando $H_0$ no se cumple**

El **Test de Cramér-von Mises (CVMT)** y el **Lilliefors (LT)** son los ganadores cuando nuestra hipótesis nula no se cumple, ya que presentan las proporciones de rechazo más altas en los distintos tamaños de muestra.

### **Comparación por tamaños de muestra**

**H_0 es verdadero**: Las diferencias entre las pruebas son mínimas (con un n pequeño o grande), ya que todas tienen una proporción de rechazos cercana al nivel de significancia (α=0.10).

**H_0 es falso**: Con n pequeñas, las diferencias pueden ser más pronunciadas debido a limitaciones en la capacidad de detectar efectos. Cuando n es más grande, las diferencias entre las proporciones de rechazo se reducen.

### **Conclusión:**

El ganador con n pequeño no necesariamente será el mismo con n grande, ya que algunas pruebas podrían necesitar más datos para alcanzar un desempeño óptimo. Al aumentar n, algunas pruebas que inicialmente eran menos potentes superan a otras que eran mejores en el inicio.

### **Impacto del tamaño de la muestra**

**Cuando H_0 se cumple**: El tamaño de la muestra tiene poco impacto en el orden de los ganadores.  

**Cuando H_0 no se cumple**: El tamaño de muestra sí puede cambiar el orden de los ganadores, ya que pruebas más robustas necesitan más datos para mostrar su precisión.

La elección de una prueba óptima depende tanto del tamaño de la muestra como de la naturaleza del problema. Para tamaños de muestra pequeños, seleccionar pruebas que sean robustas y no dependan de supuestos estrictos. Para tamaños grandes, considerar pruebas con mejor desempeño en escenarios específicos según el problema.

### **Reflexión sobre los resultados**

- En tamaños pequeños (n=30), **AD** y **SFT** tienen el mejor desempeño.  Solo los efectos grandes pueden ser detectados con confiabilidad.

- En tamaños grandes (n=480), **LT** y **AD** tienen un mejor ajuste al nivel de significancia. Incluso las diferencias o desviaciones pequeñas pueden ser detectadas, lo que puede ser útil, pero también puede llevar a rechazos de la hipótesis nula que no sean relevantes en la práctica.

- **Pruebas pequeñas (n pequeño)**: Con pocas observaciones, las pruebas tienden a ser menos sensibles para detectar desviaciones de la hipótesis nula.

- **Pruebas grandes (n grande)**: A medida que n aumenta, las pruebas se vuelven más consistentes, alineándose mejor con el nivel de significancia cuando H_0 es verdadera.

- Algunas pruebas estadísticas tienen suposiciones específicas sobre los datos y, en tamaños de muestra pequeños, estas afectan más el desempeño de las pruebas. Sin embargo, en tamaños grandes, el **Teorema Central del Límite (TLC)** asegura que las distribuciones se aproximen a una normal, haciendo que algunas pruebas sean más robustas.

### **Conclusión:**

Cuando el tamaño de muestra es pequeño, las pruebas **SFT** o **AD** son mejores, ya que suelen adaptarse mejor a escenarios con poca información. Para n grandes, aunque cualquier prueba sea efectiva, **AD** y **LT** podrían mostrar ventajas específicas dependiendo de la situación.

---

<br>

























# **Analisis de normalidad de datos**

Se desea evaluar la normalidad de un conjunto de datos contenidos en el archivo `normality_test.csv`. Para ello, se emplean visualizaciones, pruebas de bondad de ajuste y pruebas de normalidad específicas.

<br>
<center>
\( H_0: \) Los datos siguen una distribución normal. 

**\(vs\)**

\( H_a: \)
Los datos no siguen una distribución normal. 

</center>
<br>


```{r}
library(ggplot2)
library(stats)
library(nortest)

setwd("C:/Heri/GitHub/Metodos de Inferencia II/05 Trabajo Final")
datos <- read.csv("normality-test.csv", header = TRUE)
```

### **Visualización de la Distribución**

```{r}
# Histograma con densidad normal estimada
ggplot(datos, aes(x = x)) +
    geom_histogram(aes(y = ..density..), binwidth = 0.5, fill = "#1C5C3D", color = "white", size=1.5) +
    stat_function(fun = dnorm, args = list(mean = mean(datos$x), sd = sd(datos$x)), color = "#C42D2D", size = 1.5) +
    labs(title = "Histograma con Densidad Normal Estimada", x = "x", y = "Densidad") +
    theme_minimal()

```

Los datos parecen seguir un patrón cercano a una normalidad, pero con ligeros desvíos en las colas.

```{r}
# Función de Distribución Empírica (ECDF)
ggplot(datos, aes(x = x)) +
    stat_ecdf(geom = "step", size = 1.2, color = "#1C5C3D") +
    stat_function(fun = pnorm, args = list(mean = mean(datos$x), sd = sd(datos$x)), color = "#C42D2D", size = 1.2) +
    labs(title = "Función de Distribución Empírica vs Normal", x = "x", y = "Probabilidad Acumulada") +
    theme_minimal()
```

Hay ligeras discrepancias entre la distribución empírica y la normal estimada.


```{r}
# Gráfico QQ
ggplot(datos, aes(sample = x)) +
    stat_qq(color = "#FFBF00", shape = 8, size = 3 ) +
    stat_qq_line(color = "#C42D2D", 
                 size = 1.2) +
    labs(title = "Gráfico QQ", x = "Cuantiles Teóricos", y = "Cuantiles Muestrales") +
    theme_minimal()
```

Las observaciones centrales siguen una línea recta, pero hay desviaciones en las colas.




### **Partición del Espacio y Prueba de Bondad de Ajuste**

Se dividió el rango de los datos en intervalos solo con la regla de Sturges, \( \lceil 1 + \text{log}_2(80\rceil = 8 \), para crear categorías, utilizando los cuantiles. También se intentó con la regla de Rice, Scott y Freedman-Diaconis, pero daban muchas particiones, donde nuestras observaciones eran menores a 5. 

Las particiones se hicieron sobre los cuantiles, esto crea los límites de los intervalos de las clases adaptados a la distribución de los datos y aseguran que cada intervalo tenga una cantidad similar de observaciones, independientemente de los valores extremos. Luego se comparó la frecuencia observada en cada categoría con las frecuencias esperadas de una distribución normal.



```{r}

n <- 80
# Número de clases
cantidad_clases <- 8

# Calcular los límites de las clases
breaks <- quantile(datos$x, probs = seq(0, 1, length.out = cantidad_clases + 1))

# Frecuencias observadas
observados <- table(cut(datos$x, breaks = breaks, include.lowest = TRUE))

# Frecuencias esperadas
esperados <- diff(pnorm(breaks, mean = mean(datos$x), sd = sd(datos$x))) * length(datos$x)

# Normalizar las probabilidades para que sumen 1
probs <- diff(pnorm(breaks, mean = mean(datos$x), sd = sd(datos$x)))
probs <- probs / sum(probs) # Normalización

ggplot(datos, aes(x = x)) +
    geom_histogram(binwidth = 0.5, fill = "#1C5C3D", color = "white", alpha=0.6, size = 1.2) +
    geom_vline(xintercept = breaks, color = "#C42D2D", linetype = "dashed", size = 1) +
    labs(title = "Histograma con Particiones", x = "x", y = "Frecuencia") +
    theme_minimal()
```


Se añadieron líneas verticales que representan los límites de las categorías. Esto permite visualizar las discrepancias entre las frecuencias observadas y esperadas.


```{r}
# Prueba de chi-cuadrada
chisq.test(observados, p = probs)
```

Como $p = 0.3269$, no hay evidencia de que los datos no se ajustan a una distribución normal.

### **Pruebas de normalidad**

```{r}
# Cálculo manual de D- y D+
ecdf_func <- ecdf(datos$x)
d_plus <- max(ecdf_func(datos$x) - pnorm(datos$x, mean = mean(datos$x), sd = sd(datos$x)))
d_minus <- max(pnorm(datos$x, mean = mean(datos$x), sd = sd(datos$x)) - (ecdf_func(datos$x) - 1/length(datos$x)))

cat("D+ =", d_plus, "\nD- =", d_minus)

# Gráfico de distancias
plot(ecdf_func, main = "D+ y D- sobre la ECF", xlab = "x", ylab = "F(x)", pch = 8, col = "#FFBF00")
curve(pnorm(x, mean = mean(datos$x), sd = sd(datos$x)), add = TRUE, col = "#C42D2D", size = 1.5)
abline(v = datos$x[which.max(ecdf_func(datos$x) - pnorm(datos$x, mean = mean(datos$x), sd = sd(datos$x)))], col = "#3B9C38", lty = 2)
abline(v = datos$x[which.max(pnorm(datos$x, mean = mean(datos$x), sd = sd(datos$x)) - (ecdf_func(datos$x) - 1/length(datos$x)))], col = "#1C5C3D", lty = 2)

```

*(Casi no parece pero eran colores de navidad, navidad, dulce navidad)*

**Valores \(D^+\) y \(D^-\)**:

- \(D^+ = 0.11893\)

- \(D^- = 0.04572\)

- \(D = \max(D^+, D^-) = 0.11893\)

No rechazamos $H_0$. Los datos pueden seguir una distribución normal.

```{r}
ks.test(datos$x, "pnorm", mean = mean(datos$x), sd = sd(datos$x))

lillie.test(datos$x)

shapiro.test(datos$x)

```

Para un nivel de significancia del 5%, $\alpha = 0.05$:

- **Kolmogorov-Smirnov**: No rechaza la hipótesis de normalidad (\(p > 0.05\)).

- **Lilliefors**: Rechaza la hipótesis de normalidad (\(p < 0.05\)).

- **Shapiro-Francia**: También rechaza la normalidad (\(p < 0.05\)).


Las pruebas Lilliefors y Shapiro-Francia sugieren que los datos no son normales. Kolmogorov-Smirnov no detectó diferencias significativas, pero puede ser menos sensible.

Las discrepancias en los resultados pueden deberse a diferencias en la sensibilidad de las pruebas a ciertos patrones de no normalidad. Entonces, para futuros análisis tendriamos que usar métodos no paramétricos.

---

<br>




























# **Pruebas Bootstrap y Monte Carlo para Graficos QQ**

El objetivo es realizar pruebas de bondad de ajuste para dos distribuciones continuas, **Gamma** y **Lognormal**, utilizando **Monte Carlo (MC)** y **Bootstrap** en gráficos QQ. Esto permite evaluar si los datos observados siguen la distribución teórica propuesta (\(H_0\)) y discutir las diferencias entre ambas metodologías.


<br>
<center>
\( H_0: \) Los datos siguen una distribución Gamma, \(X \sim \text{Gamma}(\text{2}, \text{1})\). 

**\(vs\)**

\( H_a: \)
Los datos no siguen una distribución Gamma. 

</center>
<br>

### **Gamma**

```{r 04}

set.seed(123)
library(ggplot2)

# Parámetros
n <- 30
M <- 1000
B <- 1000
alpha <- 0.01

# Funciones para generar cuantiles teóricos
qq_theoretical <- function(data, dist = "gamma") {
  n <- length(data)
  sorted_data <- sort(data)
  if (dist == "gamma") {
    shape <- 2
    rate <- 1
    return(qgamma((1:n)/(n + 1), shape=shape, rate=rate))
  } else if (dist == "lnorm") {
    meanlog <- 0
    sdlog <- 1
    return(qlnorm((1:n)/(n + 1), meanlog=meanlog, sdlog=sdlog))
  }
}

# Alternativa Monte Carlo
monte_carlo_bands <- function(data, dist, M, alpha) {
  n <- length(data)
  theoretical <- qq_theoretical(data, dist)
  mc_quantiles <- matrix(0, nrow=M, ncol=n)
  
  for(i in 1:M) {
    if(dist == "gamma"){
      sim <- rgamma(n, shape=2, rate=1)
    } else if(dist == "lnorm"){
      sim <- rlnorm(n, meanlog=0, sdlog=1)
    }
    mc_quantiles[i, ] <- sort(sim)
  }
  
  lower <- apply(mc_quantiles, 2, quantile, probs=alpha/2)
  upper <- apply(mc_quantiles, 2, quantile, probs=1 - alpha/2)
  return(data.frame(theoretical, lower, upper))
}

# Alternativa Bootstrap
bootstrap_bands <- function(data, dist, B, alpha) {
  n <- length(data)
  sorted_data <- sort(data)
  bootstrap_quantiles <- matrix(0, nrow=B, ncol=n)
  
  for(i in 1:B) {
    indices <- sample(1:n, size=n, replace=TRUE)
    sample_data <- data[indices]
    bootstrap_quantiles[i, ] <- sort(sample_data)
  }
  
  lower <- apply(bootstrap_quantiles, 2, quantile, probs=alpha/2)
  upper <- apply(bootstrap_quantiles, 2, quantile, probs=1 - alpha/2)
  theoretical <- qq_theoretical(data, dist)
  return(data.frame(theoretical, lower, upper))
}

# Simulación para Gamma
sample_gamma <- rgamma(n, shape=2, rate=1)
qq_data_gamma <- data.frame(
  observed = sort(sample_gamma),
  theoretical = qq_theoretical(sample_gamma, "gamma")
)

mc_bands_gamma <- monte_carlo_bands(sample_gamma, "gamma", M, alpha)
bootstrap_bands_gamma <- bootstrap_bands(sample_gamma, "gamma", B, alpha)
```

```{r}
# Gráfico QQ con Monte Carlo
ggplot(qq_data_gamma, aes(x=observed, y=theoretical)) +
  geom_point(color="#C42D2D", size = 2) +
  geom_line(aes(y=mc_bands_gamma$lower), linetype="dashed", color="#1C5C3D") +
  geom_line(aes(y=mc_bands_gamma$upper), linetype="dashed", color="#1C5C3D") +
  geom_abline(intercept=0, slope=1, color = "#693A18", size = 1.5) +
  labs(title="QQ Plot Gamma - Monte Carlo", x="Cuantiles Observados", y="Cuantiles Teóricos")+
  theme_minimal()
```

Todos los puntos observados están dentro de las bandas de confianza y las bandas de confianza simuladas bajo las líneas verdes, navideñas, son más amplias a medida que los cuantiles teóricos aumentan, lo cual refleja la variabilidad esperada bajo la distribución gamma. Además, los puntos rojos, esferas navideñas, están dentro de las bandas, lo que sugiere que los datos siguen la distribución gamma. 

**Conclusión:** No hay evidencia para rechazar \(H_0\). Los datos se ajustan bien a la distribución Gamma.

```{r}
# Gráfico QQ con Bootstrap
ggplot(qq_data_gamma, aes(x=observed, y=theoretical)) +
  geom_point(color="#C42D2D", size = 2) +
  geom_line(aes(y=bootstrap_bands_gamma$lower), linetype="dashed", color="#3B9C38") +
  geom_line(aes(y=bootstrap_bands_gamma$upper), linetype="dashed", color="#3B9C38") +
  geom_abline(intercept=0, slope=1, color = "#693A18", size = 1.5) +
  labs(title="QQ Plot Gamma - Bootstrap", x="Cuantiles Observados", y="Cuantiles Teóricos")+
  theme_minimal()

```

Las bandas están en torno a los datos observados, reflejando la variabilidad empírica. Aunque los puntos observados están más cerca de la línea de identidad, hay algunas discrepancias en los extremos, lo cual también sugiere posibles desviaciones de la distribución gamma. Además, un segmento de la identidad, el tronco navideño, no está contenido dentro del intervalo de confianza.

**Conclusión:** Aunque los datos muestran ligeras desviaciones, estas no son lo suficientemente significativas para rechazar \(H_0\). La distribución Gamma sigue siendo una buena aproximación, aunque la variabilidad observada es mayor.

### **Lognormal**

<br>
<center>
\( H_0: \) Los datos siguen una distribución Lognormal, \(X \sim \text{Lognormal}(0, 1)\).

**\(vs\)**

\( H_a: \)
Los datos no siguen una distribución Lognormal. 

</center>
<br>


```{r}
# Simulación para Lognormal
sample_lnorm <- rlnorm(n, meanlog=0, sdlog=1)
qq_data_lnorm <- data.frame(
  observed = sort(sample_lnorm),
  theoretical = qq_theoretical(sample_lnorm, "lnorm")
)
```

```{r}
mc_bands_lnorm <- monte_carlo_bands(sample_lnorm, "lnorm", M, alpha)
bootstrap_bands_lnorm <- bootstrap_bands(sample_lnorm, "lnorm", B, alpha)

# Gráfico QQ con Monte Carlo
ggplot(qq_data_lnorm, aes(x=observed, y=theoretical)) +
  geom_point(color="#C42D2D", size = 2) +
  geom_line(aes(y=mc_bands_lnorm$lower), linetype="dashed", color="#1C5C3D") +
  geom_line(aes(y=mc_bands_lnorm$upper), linetype="dashed", color="#1C5C3D") +
  geom_abline(intercept=0, slope=1, color = "#693A18", size = 1.5) +
  labs(title="QQ Plot Lognormal - Monte Carlo", x="Cuantiles Observados", y="Cuantiles Teóricos")+
  theme_minimal()
```


Todos los puntos observados están dentro de las bandas de confianza 

**Conclusión:** No hay evidencia para rechazar \(H_0\). Los datos se ajustan bien a la distribución Lognormal. 

```{r}
# Gráfico QQ con Bootstrap
ggplot(qq_data_lnorm, aes(x=observed, y=theoretical)) +
  geom_point(color="#C42D2D", size = 2) +
  geom_line(aes(y=bootstrap_bands_lnorm$lower), linetype="dashed", color="#3B9C38") +
  geom_line(aes(y=bootstrap_bands_lnorm$upper), linetype="dashed", color="#3B9C38") +
  geom_abline(intercept=0, slope=1, color = "#693A18", size = 1.5) +
  labs(title="QQ Plot Lognormal - Bootstrap", x="Cuantiles Observados", y="Cuantiles Teóricos")+
  theme_minimal()

```

Un segmento de la línea de identidad no está contenido dentro de las bandas de confianza.

Las bandas de confianza son más amplias que en Monte Carlo, y las bandas de confianza son amplias, especialmente en los cuantiles superiores, reflejando la gran variabilidad en los datos extremos bajo \(H_0\). 

**Conclusión:** Las desviaciones de la línea de identidad indican mayor variabilidad en los datos observados. Sin embargo, no hay suficiente evidencia para rechazar \(H_0\). La distribución Lognormal es consistente con los datos.


### **Diferencias entre Monte Carlo y Bootstrap**

**Monte Carlo:**

- Las bandas de confianza son más ajustadas porque se construyen bajo \(H_0\).

- Detecta menos variabilidad porque asume que la distribución teórica (\(F_0\)) es correcta.

- Es más conservador para evaluar \(H_0\).

**Bootstrap:**

- Las bandas de confianza son más amplias, reflejando la variabilidad de los datos observados.

- Es más sensible a pequeñas desviaciones en la muestra, incluso si estas se deben al ruido aleatorio.

- Es más flexible y útil cuando \(H_0\) es cuestionable o los datos tienen alta variabilidad.

- Las ligeras salidas en Bootstrap pueden deberse a la variabilidad de la muestra, ya que \(n=30\), o su mayor sensibilidad a pequeños desajustes, ya que no depende de \(H_0\).
  

### **Implicaciones de las Salidas**


Comparando ambas metodologías, podemos observar que Monte Carlo es útil para probar la hipótesis nula, ya que establece un marco claro para determinar si los datos se ajustan a la distribución asumida. Mientras que, Bootstrap construye bandas basadas en los datos observados, sin asumir una distribución teórica específica y es útil para evaluar la concordancia de los datos con una distribución en términos empíricos.

Por lo tanto, para la distribución gamma, ambas metodologías indican que los datos muestran cierta discrepancia, particularmente en los extremos. La metodología Bootstrap parece reflejar mejor las desviaciones empíricas, mientras que Monte Carlo da una indicación más clara en relación con la hipótesis. De igual manera para la muestra Lognormal, tanto Monte Carlo como Bootstrap proporcionan evidencia sólida de que los datos no se ajustan bien a la distribución, sin embargo, Monte Carlo, muestra desviaciones más marcadas en los cuantiles superiores.

Concluyendo que Monte Carlo es más adecuado si el objetivo principal es probar la hipótesis de manera formal, ya que proporciona un marco teórico claro y Bootstrap es preferible si se quiere evaluar la concordancia empírica de los datos con la distribución asumida y hay alta variabilidad en la información o se trabaja con distribuciones complejas.

- **Monte Carlo** es preferible si se tiene confianza en \(H_0\) o si se requiere un análisis más conservador.

- **Bootstrap** es útil en casos donde los datos presentan alta variabilidad o \(H_0\) es cuestionable.

Ambas metodologías respaldan \(H_0\) para las distribuciones Gamma y Lognormal. Sin embargo, las desviaciones observadas en Bootstrap reflejan que los datos tienen mayor variabilidad de lo que sugieren las bandas construidas bajo \(H_0\) en Monte Carlo. En escenarios con muestras pequeñas o alta variabilidad, **Bootstrap** puede ser más informativo.


---

<br>




























# **Análisis de Emisiones de CO2: E.E.U.U. vs México**

Este ejercicio evalúa si las emisiones de CO2 en los Estados Unidos son estocásticamente mayores que en México desde 1930 en adelante. Se utilizan pruebas estadísticas no paramétricas y paramétricas para analizar las diferencias entre ambos países.


### **Filtrado y Limpieza** 
**Filtrado por Año**: Se incluyeron solo las observaciones desde 1930 en adelante.

**Limpieza de Datos**: Se eliminaron las observaciones con valores de 0 y las entradas vacías para evitar sesgos.

### **Prueba no paramétrica**

<br>
<center>
\(H_0\): No hay diferencia en la distribución de emisiones de CO2 entre E.E.U.U. y México.

**\(vs\)**

\(H_a\): Las emisiones de CO2 en E.E.U.U. son estocásticamente mayores que en México.
</center>
<br>


```{r}
# Filtrar los datos para incluir solo las observaciones desde 1930 en adelante
setwd("C:/Heri/GitHub/Metodos de Inferencia II/05 Trabajo Final")
data <- read.csv("CO2-emissions.csv", stringsAsFactors = FALSE)
data_filtered <- subset(data, Year >= 1930)

# Ya tenia esta en Python
rank_function <- function(list1, list2) {
  
  combined <- c(list1, list2)
  sorted_combined <- sort(combined)
  
  sorted_list1 <- sort(list1)
  sorted_list2 <- sort(list2)
  
  matrix1 <- data.frame(Value = sorted_list1, Type = "x", stringsAsFactors = FALSE)
  matrix2 <- data.frame(Value = sorted_list2, Type = "y", stringsAsFactors = FALSE)
  
  matrix <- rbind(matrix1, matrix2)
  matrix <- matrix[order(matrix$Value), ]
  
  sorted_matrix <- matrix
  sorted_matrix$Rank <- 1:nrow(sorted_matrix)
  
  sorted_matrix$Count <- ifelse(sorted_matrix$Type == "x", 
                                sapply(1:nrow(sorted_matrix), function(i) sum(sorted_matrix$Type[1:(i-1)] == "y" & sorted_matrix$Value[1:(i-1)] < sorted_matrix$Value[i])), 
                                0)
  
  header <- "Value\tType\tRank\tCount\n"
  result <- apply(sorted_matrix, 1, function(row) {
    paste(row["Value"], row["Type"], row["Rank"], row["Count"], sep = "\t")
  })
  
  sum_ranks <- sum(sorted_matrix$Rank[sorted_matrix$Type == "x"])
  sum_counts <- sum(sorted_matrix$Count[sorted_matrix$Type != "-"])
  
  sum_ranks_msg <- paste("\nsum_ranks list1 > list2 =", sum_ranks, "\n")
  sum_counts_msg <- paste("sum_counts =", sum_counts, "\n")
  
  return(sum_counts)
}

# Wilcoxon Rank Sum Test
usa <- data_filtered$USA
mexico <- data_filtered$Mexico
sum_ranks <- rank_function(usa, mexico)
n1 <- length(usa)
n2 <- length(mexico)
mu_U <- n1 * n2 / 2
sigma_U <- sqrt(n1 * n2 * (n1 + n2 + 1) / 12)
z <- (sum_ranks - mu_U) / sigma_U
p_value <- 1 - pnorm(z)

cat("Prueba Wilcoxon manual:\n\n  W =", sum_ranks, "\n", " p-valor =", p_value, "\n")

# Comparar con wilcox.test
EEUU <- usa
Mexico <- mexico
wilcox.test(EEUU, Mexico, alternative = "greater")
```

Tenemos un p-valor muy bajo, \(p < 2.2 \times 10^{-16}\), entonces se rechaza \(H_0\). Las emisiones de CO2 en E.E.U.U. son significativamente mayores que en México.

### **Diferencia de Medias**

<br>
<center>
\(H_0\): La media de emisiones de CO2 en E.E.U.U. es igual a la media en México.

**\(vs\)**

\(H_a\): La media de emisiones de CO2 en E.E.U.U. es mayor que en México.

</center>
<br>

```{r}
# Filtrar los datos para incluir solo las observaciones desde 1930 en adelante
setwd("C:/Heri/GitHub/Metodos de Inferencia II/05 Trabajo Final")
data <- read.csv("CO2-emissions.csv", stringsAsFactors = FALSE)
data_filtered <- subset(data, Year >= 1930)

usa <- data_filtered$USA

mexico <- data_filtered$Mexico


EEUU <- usa
Mexico <- mexico

# Prueba t de manera manual
mean_usa <- mean(usa)
mean_mexico <- mean(mexico)
var_usa <- var(usa)
var_mexico <- var(mexico)
t_stat <- (mean_usa - mean_mexico) / sqrt(var_usa/n1 + var_mexico/n2)
df <- (var_usa/n1 + var_mexico/n2)^2 / ((var_usa^2)/(n1^2*(n1-1)) + (var_mexico^2)/(n2^2*(n2-1)))
p_manual <- 1 - pt(t_stat, df)

cat("Prueba t para dos medias manual:\n\n  t =", t_stat, "\n  df =", df , "\n  p-valor =", p_manual,
"\n  Media EEUU =", mean_usa, "\n  Media Mexico =", mean_mexico, "\n")

# Prueba t para diferencia de medias
t.test(EEUU, Mexico, alternative = "greater", var.equal = FALSE)
```

**Supuestos de la Prueba t**  

**Normalidad**: Con grandes tamaños de muestra, el teorema central del límite puede justificar la normalidad.  

**Homogeneidad de Varianzas**: Se utilizó la versión de Welch, que no asume varianzas iguales.

Volvemos a tener un $p-valor$ muy bajo, rechazamos $H_0$. La media de emisiones de CO2 en E.E.U.U. es mayor que en México.


### **Mann-Whitney vs Prueba t**:

- Ambas pruebas indican que las emisiones de E.E.U.U. son mayores que las de México.

- La prueba t mide diferencias en medias, mientras que Mann-Whitney evalúa diferencias en la distribución.

- Si los supuestos de la prueba t no se cumplen (e.g., normalidad), el método Bootstrap puede proporcionar una estimación robusta.



### **Conclusiones**

- Las emisiones de CO2 en E.E.U.U. son significativamente mayores que en México.

- Las pruebas no paramétricas y t producen resultados consistentes, aunque evalúan propiedades diferentes de los datos.

- En situaciones donde no se cumplen los supuestos de la prueba t, el Bootstrap o pruebas no paramétricas son alternativas adecuadas.

---

<br>



























# **Efecto de una Dieta Especial sobre el Colesterol**

Se evaluó el efecto de una dieta especial con margarina Clora en los niveles de colesterol de participantes. Se midieron los niveles de colesterol antes de la dieta, después de 4 semanas y después de 8 semanas. 
El objetivo es determinar si la dieta produjo un efecto significativo utilizando una prueba no paramétrica adecuada.

### **Prueba de Hipótesis**


<br>
<center>
\(H_0: \) No hay diferencia significativa entre los niveles de colesterol antes de la dieta y después de 4 semanas (\(m_{\text{antes}} = m_{\text{después}}\)).

**\(vs\)**

\(H_a: \) Hay una diferencia significativa entre los niveles de colesterol antes de la dieta y después de 4 semanas (\(m_{\text{antes}} \neq m_{\text{después4}}\)).

</center>
<br>

### **Selección de la Prueba**
Por qué no usar la prueba de Mann-Whitney: Esta prueba compara dos muestras independientes, mientras que los datos en este estudio son pares (niveles de colesterol antes y después de la dieta son del mismo sujeto).
La prueba de **Wilcoxon Signed Rank Test** es adecuada, ya que evalúa diferencias en datos relacionados.

```{r}

antes <- c(6.42, 6.76, 6.56, 4.80, 8.43, 7.49, 8.05, 5.05, 5.77, 3.91, 6.77, 6.44, 6.17, 7.67, 7.34, 6.85, 5.13, 5.73)
despues4 <- c(5.83, 6.20, 5.83, 4.27, 7.71, 7.12, 7.25, 4.63, 5.31, 3.70, 6.15, 5.59, 5.56, 7.11, 6.84, 6.40, 4.52, 5.13)

signs <- antes - despues4
n <- length(signs)
W <- n*(n+1)/2
E_x <- n*(n+1)/4
V_x <- n*(n+1)*(2*n+1)/24
z <- (W - 0.5 - E_x) / sqrt(V_x)
p_value <- 2 * (1 - pnorm(abs(z)))

cat("WSRT Manual:\n  W =", W, "\n  E[x] = ", E_x, "\n  V[x] = ", V_x, "\n  Z = ", z,  "\n  p-valor =", p_value, "\n")

# Prueba de Wilcoxon
wilcox.test(antes, despues4, paired = TRUE, alternative = "two.sided")

```


### **Decisión**
Dado que \(p = 0.0002134\), se rechaza la hipótesis nula. Hay evidencia estadísticamente significativa de que la dieta afecta los niveles de colesterol en los participantes.


### **Conclusiones**
La dieta especial con margarina Clora produjo un efecto significativo en los niveles de colesterol después de 4 semanas.

Los resultados sugieren una disminución en los niveles de colesterol (mediana de las diferencias), aunque este análisis no cuantifica la magnitud del cambio.

### **Comparación con la Prueba de Wilcoxon**

Al realizar la prueba correctamente como pareada:

- La prueba no paramétrica considera las relaciones dentro de los pares, lo cual es más adecuado para este tipo de diseño.

- La prueba de Mann-Whitney, si se hubiera usado, no habría aprovechado la información de las dependencias dentro de los datos y podría haber llevado a conclusiones incorrectas.



























</div>
